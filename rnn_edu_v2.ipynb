{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN Modeling of Behavior and Performance\n",
    "\n",
    "- In this notebook, we will be experimenting with two models: skill2skill and DKT (Deep Knowledge Tracing). \n",
    "- Starter code for both the models has already been provided to you. \n",
    "- See below for how to initiate and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "filename = 'skill_builder_data_corrected.csv'\n",
    "df = pd.read_csv(filename, encoding='ISO-8859-1', low_memory=False)\n",
    "df = df[(df['original'] == 1) & (df['attempt_count'] == 1) & ~(df['skill_name'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = pd.read_csv('correct.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "skill_df = pd.read_csv('skill.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "assistment_df = pd.read_csv('assistment_id.tsv', sep='\\t').drop('Unnamed: 0', axis=1)\n",
    "skill_dict = {}\n",
    "with open('skill_dict.json', 'r', encoding='utf-8') as f:\n",
    "    loaded = json.load(f)\n",
    "    for k, v in loaded.items():\n",
    "        skill_dict[k] = int(v)\n",
    "\n",
    "skill_num = len(skill_dict) + 1 # including 0\n",
    "\n",
    "def one_hot(skill_matrix, vocab_size):\n",
    "    '''\n",
    "    params:\n",
    "        skill_matrix: 2-D matrix (student, skills)\n",
    "        vocal_size: size of the vocabulary\n",
    "    returns:\n",
    "        a ndarray with a shape like (student, sequence_len, vocab_size)\n",
    "    '''\n",
    "    seq_len = skill_matrix.shape[1]\n",
    "    result = np.zeros((skill_matrix.shape[0], seq_len, vocab_size))\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        result[i, np.arange(seq_len), skill_matrix[i]] = 1.\n",
    "    return result\n",
    "\n",
    "def dkt_one_hot(skill_matrix, response_matrix, vocab_size):\n",
    "    seq_len = skill_matrix.shape[1]\n",
    "    skill_response_array = np.zeros((skill_matrix.shape[0], seq_len, 2 * vocab_size))\n",
    "    for i in range(skill_matrix.shape[0]):\n",
    "        skill_response_array[i, np.arange(seq_len), 2 * skill_matrix[i] + response_matrix[i]] = 1.\n",
    "    return skill_response_array\n",
    "\n",
    "def preprocess(skill_df, response_df, skill_num):\n",
    "    skill_matrix = skill_df.iloc[:, 1:].values\n",
    "    response_array = response_df.iloc[:, 1:].values\n",
    "    skill_array = one_hot(skill_matrix, skill_num)\n",
    "    skill_response_array = dkt_one_hot(skill_matrix, response_array, skill_num)\n",
    "    return skill_array, response_array, skill_response_array\n",
    "    \n",
    "\n",
    "skill_array, response_array, skill_response_array = preprocess(skill_df, response_df, skill_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, Lambda, multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "\n",
    "def build_skill2skill_model(input_shape, lstm_dim=32, dropout=0.0):\n",
    "    input = Input(shape=input_shape, name='input skills')\n",
    "    lstm = LSTM(lstm_dim, \n",
    "                return_sequences=True, \n",
    "                dropout=dropout,\n",
    "                name='lstm layer')(input)\n",
    "    output = TimeDistributed(Dense(input_shape[-1], activation='softmax'), name='probability')(lstm)\n",
    "    model = Model(inputs=[input], outputs=[output])\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def reduce_dim(x):\n",
    "    x = K.max(x, axis=-1, keepdims=True)\n",
    "    return x\n",
    "\n",
    "def build_dkt_model(input_shape, lstm_dim=32, dropout=0.0):\n",
    "    input_skills = Input(shape=input_shape, name='input skills')\n",
    "    lstm = LSTM(lstm_dim, \n",
    "                return_sequences=True, \n",
    "                dropout=dropout,\n",
    "                name='lstm layer')(input_skills)\n",
    "    dense = TimeDistributed(Dense(int(input_shape[-1]/2), activation='sigmoid'), name='probability for each')(lstm)\n",
    "    \n",
    "    skill_next = Input(shape=(input_shape[0], int(input_shape[1]/2)), name='next_skill_tested')\n",
    "    merged = multiply([dense, skill_next], name='multiply')\n",
    "    reduced = Lambda(reduce_dim, output_shape=(input_shape[0], 1), name='reduce dim')(merged)\n",
    "    \n",
    "    model = Model(inputs=[input_skills, skill_next], outputs=[reduced])\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample Usage: skill2skill\n",
    "print('skill2skill')\n",
    "skill2skill_model = build_skill2skill_model((99, skill_num), lstm_dim=64)\n",
    "\n",
    "# Sample Usage: DKT\n",
    "print('dkt')\n",
    "dkt_model = build_dkt_model((99, 2 * skill_num), lstm_dim=64)\n",
    "    \n",
    "\n",
    "# train skill2skill\n",
    "skill2skill_model.fit(skill_array[:, 0:-1], \n",
    "                      skill_array[:, 1:],\n",
    "                      epochs=20, \n",
    "                      batch_size=32, \n",
    "                      shuffle=True,\n",
    "                      validation_split=0.2)\n",
    "\n",
    "# train DKT \n",
    "dkt_model.fit([skill_response_array[:, 0:-1], skill_array[:, 1:]],\n",
    "              response_array[:, 1:, np.newaxis],\n",
    "              epochs=20, \n",
    "              batch_size=32, \n",
    "              shuffle=True,\n",
    "              validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on Q2\n",
    "\n",
    "Question 2 of the RNN homework ask you to report on the hardest and easiest to predict skills using the skill2skill model. In order to establish some sortable measure of prediction difficulty, you need to come up with a metric that represents this prediction difficulty. Here are a few suggestions for metrics\n",
    "\n",
    "1) Accuracy: For each skill, what percent of the time (when it was the label) was it predicted correctly? One approach to this is to create a binary array for each skill, whose length is equal to the number of times the skill appears as a label. When that skill was the label and had the highest probability in the softmax output, the array value is 1, otherwise 0. The average value of this array is the accuracy of prediction for that skill. The finaly step is then to sort all skills by this average accuracy. \n",
    "\n",
    "2) Recall @ N: This is the same idea as above, except that you would look to see if the probability associated with the label's skill was among the top N probabilities. Say, for N = 5, if the label's probability was in the top N, the array value would be true, and false otherwise. The average of the array would then be the Recall @ N for that skill. This metric should always return a value equal to or higher  than accuracy.\n",
    "\n",
    "3) Average probability: Instead of recording if the probability was the best, you could record only the probability itself. The skills will the lowest average probability is one way of characterizing them as being hardest (or least likely) to predict. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
